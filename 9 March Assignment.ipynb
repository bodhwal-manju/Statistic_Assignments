{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b28252d-65fd-4279-a5ab-8abb16611870",
   "metadata": {},
   "source": [
    "# Q1. What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458438c3-f28e-4d28-9053-9ba41c6c109b",
   "metadata": {},
   "source": [
    "1.Probability Mass Function (PMF):\n",
    "A Probability Mass Function (PMF) is used to describe the probability distribution of a discrete random variable. It gives the probability of each possible outcome in the sample space. The PMF is defined as the function that assigns probabilities to each possible value of the discrete random variable.\n",
    "\n",
    "\n",
    "For example, consider a fair six-sided die. The PMF of this die would give the probabilities associated with each face of the die, which are all equal to 1/6.The PMF provides a complete description of the probabilities associated with each possible outcome of the discrete random variable.\n",
    "\n",
    "\n",
    "2.Probability Density Function (PDF):\n",
    "A Probability Density Function (PDF) is used to describe the probability distribution of a continuous random variable. It gives the probability density at each point in the sample space. Unlike the PMF, which assigns probabilities to specific values, the PDF describes the relative likelihood of a continuous random variable taking on different values within a given interval.\n",
    "\n",
    "For example, consider the standard normal distribution. The PDF of the standard normal distribution is the bell-shaped curve. It describes the relative likelihood of the random variable taking on different values along the x-axis. The PDF is a smooth curve, and the area under the curve within a specific interval represents the probability of the random variable falling within that interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a926f-b8f2-44ed-bdd9-272d89edac11",
   "metadata": {},
   "source": [
    "# Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ef2ca-16c2-4eae-970f-bb75578875bf",
   "metadata": {},
   "source": [
    "The Cumulative Density Function (CDF) is a concept used in probability theory and statistics to describe the cumulative probability distribution of a random variable. The CDF gives the probability that a random variable takes on a value less than or equal to a given value.\n",
    "\n",
    "Formally, the CDF of a random variable X, denoted as F(x), is defined as:\n",
    "\n",
    "F(x) = P(X ≤ x)\n",
    "\n",
    "In other words, the CDF at a specific value x gives the probability that the random variable X is less than or equal to x.\n",
    "\n",
    "Here's an example to illustrate the concept of the CDF:\n",
    "\n",
    "Consider a fair six-sided die. The CDF of this die would give the cumulative probabilities associated with each possible face of the die. The CDF would look like this:\n",
    "\n",
    " X       1\t2\t3\t4\t5\t6\n",
    " _____________________________\n",
    "F(X)\t1/6\t2/6\t3/6\t4/6\t5/6 1\n",
    "\n",
    "The CDF tells us that the probability of rolling a number less than or equal to 3 is 3/6, or 0.5. Similarly, the probability of rolling a number less than or equal to 5 is 5/6, and the probability of rolling a number less than or equal to 6 is 1.\n",
    "\n",
    "The CDF is used for several reasons:\n",
    "\n",
    "1.Probability Calculation: The CDF allows us to calculate probabilities associated with a random variable without having to calculate individual probabilities for each specific value. By providing the cumulative probabilities, it gives us a straightforward way to determine the probability of a random variable falling within a certain range.\n",
    "\n",
    "2.Distribution Characteristics: The shape and properties of the CDF can provide valuable insights into the distribution of a random variable. For example, it can help identify important percentiles, such as the median (50th percentile) or quartiles, which can be useful for making statistical inferences or making decisions based on the data.\n",
    "\n",
    "3.Comparisons: The CDF enables comparisons between different random variables or different distributions. By comparing the CDFs of two random variables or distributions, we can determine which one has a higher probability of being larger or smaller.\n",
    "\n",
    "4.Random Variable Transformation: The CDF plays a crucial role in transforming random variables. It helps in transforming a random variable from one distribution to another, such as generating random numbers with a desired distribution using techniques like inverse transform sampling or Monte Carlo simulation.\n",
    "\n",
    "In summary, the Cumulative Density Function (CDF) provides the cumulative probabilities associated with a random variable. It allows us to calculate probabilities, understand the distribution characteristics, make comparisons, and perform transformations.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bfb097-8bda-4cc5-b864-5790abab9695",
   "metadata": {},
   "source": [
    "# Q3: What are some examples of situations where the normal distribution might be used as a model? Explain how the parameters of the normal distribution relate to the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326bd84-abce-4858-a866-63d9e15e00d9",
   "metadata": {},
   "source": [
    "The normal distribution, also known as the Gaussian distribution, is widely used as a model in various fields due to its numerous applications and desirable properties. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "1.Natural Phenomena: Many natural phenomena exhibit a distribution that can be approximated by the normal distribution. Examples include the heights and weights of individuals in a population, IQ scores, measurement errors, and the distribution of blood pressure or body temperature in a healthy population.\n",
    "\n",
    "2.Financial Data: In finance and economics, the normal distribution is often used to model stock prices, asset returns, and other financial variables. It serves as a foundation for various financial models, such as the Black-Scholes option pricing model.\n",
    "\n",
    "3.Quality Control: The normal distribution is frequently employed in quality control processes to model the distribution of product characteristics. For example, the distribution of the weights of manufactured items or the diameters of screws can be approximated by a normal distribution.\n",
    "\n",
    "4.Experimental Data: In many scientific experiments, the normal distribution is used to model the distribution of measurement errors. It is assumed that these errors follow a normal distribution, allowing researchers to make statistical inferences and draw conclusions about the underlying phenomena being studied.\n",
    "\n",
    "The normal distribution is characterized by two parameters: the mean (μ) and the standard deviation (σ). These parameters play a crucial role in determining the shape of the distribution:\n",
    "\n",
    "Mean (μ): The mean of a normal distribution represents the center or average value of the distribution. It determines the location of the peak or the point of symmetry. Shifting the mean to the left or right causes the entire distribution to shift accordingly.\n",
    "\n",
    "Standard Deviation (σ): The standard deviation of a normal distribution measures the spread or variability of the data. A larger standard deviation results in a wider distribution, while a smaller standard deviation yields a narrower distribution. It affects the width of the curve and determines the probability of observing values within a certain range from the mean.\n",
    "\n",
    "By manipulating the mean and standard deviation, we can adjust the position and shape of the normal distribution to better fit the data at hand. This flexibility allows the normal distribution to be widely used as a model in many practical applications. Additionally, the Central Limit Theorem further supports the use of the normal distribution as an approximation for the distribution of sample means, making it valuable in statistical inference.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f556a1-2199-4c1c-b607-195b7f2d0bc6",
   "metadata": {},
   "source": [
    "# Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90fd27-07e5-44d7-b604-d2c7a6e6ad2e",
   "metadata": {},
   "source": [
    "The normal distribution, also known as the Gaussian distribution, holds significant importance in various fields due to its numerous practical applications. Here are some key reasons for the importance of the normal distribution:\n",
    "\n",
    "1.Common Natural Distribution: The normal distribution is often found to describe the distribution of many natural phenomena in the real world. This is due to the Central Limit Theorem, which states that the sum or average of a large number of independent and identically distributed random variables tends to follow a normal distribution, regardless of the underlying distribution of the individual variables. As a result, the normal distribution is a convenient approximation for many real-life scenarios.\n",
    "\n",
    "2.Statistical Inference: The normal distribution plays a crucial role in statistical inference. Many statistical methods, such as hypothesis testing and confidence intervals, rely on the assumption of a normal distribution. When the sample size is large or when the Central Limit Theorem applies, the normal distribution is used to make inferences about population parameters.\n",
    "\n",
    "3.Predictive Modeling: In predictive modeling and machine learning, the normal distribution is frequently used as a foundational assumption. It forms the basis for several regression techniques, such as linear regression, where the errors are assumed to follow a normal distribution. Additionally, various algorithms, such as Gaussian Naive Bayes and Gaussian Mixture Models, are specifically designed for data that adheres to a normal distribution.\n",
    "\n",
    "Real-life examples of situations where the normal distribution is observed include:\n",
    "\n",
    "1.Heights and Weights: The distribution of human heights and weights within a population tends to follow a normal distribution. While there may be slight deviations from perfect normality, the overall trend is often well-approximated by the normal distribution.\n",
    "\n",
    "2.Exam Scores: In educational assessments, the distribution of exam scores among a large population of students often exhibits a normal distribution. This allows for the calculation of percentiles and the determination of performance relative to the population.\n",
    "\n",
    "3.IQ Scores: Intelligence quotient (IQ) scores are often assumed to follow a normal distribution, with the mean set at 100 and a standard deviation of 15. This assumption facilitates the interpretation and comparison of individuals' scores.\n",
    "\n",
    "4.Measurement Errors: In scientific experiments and industrial processes, measurement errors are often assumed to be normally distributed. This assumption allows for the estimation of uncertainties, calibration, and decision-making based on the observed data.\n",
    "\n",
    "5.Financial Data: In finance, stock returns, asset prices, and other financial variables are often modeled as normally distributed. This assumption forms the foundation for various financial models and risk assessments.\n",
    "\n",
    "It is important to note that while the normal distribution is widely applicable, it may not always be the best fit for every situation. Other distributions, such as the Poisson or exponential distributions, may be more appropriate in specific contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5506d-c931-4d41-8bde-ea86dfc87751",
   "metadata": {},
   "source": [
    "# Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli Distribution and Binomial Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857de00-2143-4151-854f-c8acf0fdb29e",
   "metadata": {},
   "source": [
    "The Bernoulli distribution is a discrete probability distribution that models a random experiment with two possible outcomes: success (usually denoted as 1) and failure (usually denoted as 0). It is named after Jacob Bernoulli, a Swiss mathematician.\n",
    "\n",
    "The Bernoulli distribution is characterized by a single parameter, often denoted as p, which represents the probability of success. The probability mass function (PMF) of the Bernoulli distribution is given by:\n",
    "\n",
    "P(X = k) = p^k * (1 - p)^(1 - k)\n",
    "\n",
    "where X is a random variable following a Bernoulli distribution, k can be either 0 or 1, and p is the probability of success.\n",
    "\n",
    "An example of the Bernoulli distribution is modeling the outcome of a coin flip. Let's assume that we have a fair coin, and we are interested in the probability of getting heads (success). In this case, p = 0.5, as both heads and tails have an equal chance of occurring. The Bernoulli distribution for this scenario can be represented as:\n",
    "\n",
    "P(X = 1) = 0.5\n",
    "P(X = 0) = 0.5\n",
    "\n",
    "Now, let's move on to the difference between the Bernoulli distribution and the binomial distribution:\n",
    "\n",
    "Bernoulli Distribution: The Bernoulli distribution models a single experiment with two possible outcomes: success or failure. It is used when there is only one trial involved.\n",
    "\n",
    "Binomial Distribution: The binomial distribution is an extension of the Bernoulli distribution and models a series of independent and identical Bernoulli trials. It represents the number of successes in a fixed number of independent trials, each with the same probability of success. The binomial distribution has two parameters: n (the number of trials) and p (the probability of success in each trial).\n",
    "\n",
    "The relationship between the Bernoulli distribution and the binomial distribution is that the binomial distribution can be thought of as the sum of multiple independent and identically distributed Bernoulli random variables.\n",
    "\n",
    "For example, consider flipping a fair coin 10 times and counting the number of heads. Each individual coin flip follows a Bernoulli distribution with p = 0.5 (probability of success, i.e., getting a head). The number of heads obtained after 10 flips follows a binomial distribution with parameters n = 10 (number of trials) and p = 0.5 (probability of success in each trial).\n",
    "\n",
    "In summary, the Bernoulli distribution models a single trial with two outcomes (success or failure), while the binomial distribution models the number of successes in a fixed number of independent and identical trials, each following a Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8540523-67ad-4f2e-9b3e-af4840129b27",
   "metadata": {},
   "source": [
    "# Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset is normally distributed, what is the probability that a randomly selected observation will be greater than 60? Use the appropriate formula and show your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c951d4-72f7-48ef-b9a5-708f0fe94c82",
   "metadata": {},
   "source": [
    "To calculate the probability that a randomly selected observation from a normally distributed dataset with a mean of 50 and a standard deviation of 10 is greater than 60, we can use the Z-score and the standard normal distribution.\n",
    "\n",
    "The Z-score is a measure of how many standard deviations an observation is from the mean. In this case, we want to calculate the Z-score for the value 60. The formula to calculate the Z-score is:\n",
    "\n",
    "Z = (X - μ) / σ\n",
    "\n",
    "where X is the value of interest, μ is the mean, and σ is the standard deviation.\n",
    "\n",
    "Let's calculate the Z-score for 60:\n",
    "\n",
    "Z = (60 - 50) / 10\n",
    "Z = 1\n",
    "\n",
    "The Z-score tells us that the value 60 is 1 standard deviation above the mean.\n",
    "\n",
    "Now, we can use the standard normal distribution table or a calculator to find the probability associated with the Z-score of 1. In this case, we want to find the probability of being greater than 1 standard deviation above the mean.\n",
    "\n",
    "Looking up the Z-table or using a calculator, we find that the probability of a Z-score greater than 1 is approximately 0.1587.\n",
    "\n",
    "Therefore, the probability that a randomly selected observation from the given normally distributed dataset will be greater than 60 is approximately 0.1587, or 15.87%.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c001ea6-825a-46c2-8e40-6745506cc559",
   "metadata": {},
   "source": [
    "# Q7: Explain uniform Distribution with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4cb144-2562-42cd-ab56-496c7942074b",
   "metadata": {},
   "source": [
    "The uniform distribution is a continuous probability distribution that models a situation where all outcomes within a given range are equally likely. In other words, it represents a scenario where every possible value has the same probability of occurring.\n",
    "\n",
    "In the uniform distribution, the probability density function (PDF) is constant within the specified range and zero outside that range. The PDF is defined as:\n",
    "\n",
    "f(x) = 1 / (b - a)\n",
    "\n",
    "where a is the lower bound of the range and b is the upper bound of the range.\n",
    "\n",
    "Here's an example to illustrate the uniform distribution:\n",
    "\n",
    "Consider rolling a fair six-sided die. In this case, the outcomes are numbers from 1 to 6, and each outcome has an equal probability of 1/6. The uniform distribution can be represented by the following PDF:\n",
    "\n",
    "f(x) = 1/6 for x = 1, 2, 3, 4, 5, 6\n",
    "f(x) = 0 otherwise\n",
    "\n",
    "The probability density function is constant at 1/6 for each possible outcome (1, 2, 3, 4, 5, 6) and zero for any value outside that range.\n",
    "\n",
    "In this example, the uniform distribution reflects the fact that each number on the die has an equal likelihood of being rolled. There is no preference or bias towards any particular number.\n",
    "\n",
    "Another example is selecting a random number between 0 and 1. In this case, any value between 0 and 1 has an equal probability of occurring. The PDF of the uniform distribution in this scenario is:\n",
    "\n",
    "f(x) = 1 for 0 ≤ x ≤ 1\n",
    "f(x) = 0 otherwise\n",
    "\n",
    "The uniform distribution is often used in simulations, random number generation, and situations where all outcomes are equally likely. It provides a simple and fair representation of randomness within a defined range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d31678-1740-42c2-b3ea-45ed706d4beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "#calculate uniform probability\n",
    "uniform.cdf(x=8, loc=0, scale=20) - uniform.cdf(x=0, loc=0, scale=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047e160-c77e-4c38-9353-234552821a44",
   "metadata": {},
   "source": [
    "# Q8: What is the z score? State the importance of the z score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffbfbe2-1d11-4146-9386-4361c5017dc3",
   "metadata": {},
   "source": [
    "The Z-score, also known as the standard score, is a measure that indicates how many standard deviations an individual data point or observation is away from the mean of a distribution. It standardizes the values of a dataset, allowing for comparisons and analysis across different distributions.\n",
    "\n",
    "The formula to calculate the Z-score for a data point X in a distribution with mean μ and standard deviation σ is:\n",
    "\n",
    "Z = (X - μ) / σ\n",
    "\n",
    "The Z-score has several important uses and significance:\n",
    "\n",
    "1.Standardization: The Z-score standardizes data, transforming it into a common scale with a mean of 0 and a standard deviation of 1. This standardization allows for the comparison of values from different distributions or variables with different scales. It is particularly useful when comparing data points from different populations or when conducting statistical analyses.\n",
    "\n",
    "2.Relative Position: The Z-score provides information about the relative position of a data point within a distribution. Positive Z-scores indicate that the data point is above the mean, while negative Z-scores indicate it is below the mean. The magnitude of the Z-score indicates how far away the data point is from the mean in terms of standard deviations.\n",
    "\n",
    "3.Probability Calculation: The Z-score can be used to calculate probabilities associated with specific values or ranges in a normal distribution. By referring to a standard normal distribution table or using statistical software, the Z-score allows us to determine the probability of observing a value below, above, or between specific Z-score values. This is particularly useful for hypothesis testing and determining confidence intervals.\n",
    "\n",
    "4.Outlier Detection: Z-scores can be used to identify outliers in a dataset. Data points with Z-scores significantly greater or smaller than 0 are considered unusual or extreme compared to the rest of the data. By setting a threshold, such as |Z| > 2 or |Z| > 3, outliers can be identified and investigated further.\n",
    "\n",
    "5.Data Transformation: Z-scores can be employed to transform data to meet assumptions of normality or to normalize skewed distributions. By converting data to Z-scores, it is possible to assess the normality of a distribution, apply statistical techniques that assume normality, or compare data points across different variables or populations.\n",
    "\n",
    "In summary, the Z-score is a standardized measure that allows for comparisons, probability calculations, and outlier detection. It is a valuable tool in statistics and data analysis, providing insights into the relative position and characteristics of individual data points within a distribution.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ca2a2-73e3-430c-8e0c-d81bd4d552ce",
   "metadata": {},
   "source": [
    "# Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb2ffe5-b48a-48fd-a7ed-f15ad263bea0",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental concept in statistics that states that the sampling distribution of the mean of a large number of independent and identically distributed random variables will be approximately normally distributed, regardless of the shape of the original population distribution, as long as certain conditions are met.\n",
    "\n",
    "The Central Limit Theorem has the following key characteristics:\n",
    "\n",
    "1.Sample Size: The CLT holds as the sample size increases. As the sample size becomes larger, the sampling distribution of the mean becomes increasingly close to a normal distribution.\n",
    "\n",
    "2.Independence: The random variables in the sample must be independent, meaning that the value of one variable does not influence the value of another. This assumption is crucial for the CLT to hold.\n",
    "\n",
    "3.Identical Distribution: The random variables in the sample must be identically distributed, meaning that they are drawn from the same population with the same underlying distribution.\n",
    "\n",
    "The significance of the Central Limit Theorem lies in its broad applications and implications in statistics and data analysis:\n",
    "\n",
    "1.Normal Approximation: The CLT allows us to approximate the distribution of the sample mean by a normal distribution, even if the original population distribution is not normal. This is particularly useful when making inferences about the population mean or constructing confidence intervals.\n",
    "\n",
    "2.Statistical Inference: The CLT provides the basis for many statistical techniques and methods, such as hypothesis testing and confidence interval estimation. It enables the use of parametric tests and estimation procedures even when the underlying population distribution is unknown or not normally distributed.\n",
    "\n",
    "3.Sample Size Determination: The CLT helps in determining the appropriate sample size needed for statistical analyses. By knowing that the sampling distribution of the mean approaches a normal distribution as the sample size increases, we can determine the required sample size to achieve a desired level of precision in estimation or hypothesis testing.\n",
    "\n",
    "4.Population Inference: The CLT allows us to make inferences about population parameters, such as the mean, even when the population distribution is unknown or not normal. It provides a framework for generalizing from sample statistics to population parameters.\n",
    "\n",
    "5.Data Analysis: The CLT provides a justification for using statistical techniques that assume normality, such as t-tests and analysis of variance (ANOVA). It allows us to make valid inferences and draw conclusions based on these techniques, even if the original data is not normally distributed.\n",
    "\n",
    "Overall, the Central Limit Theorem is a fundamental concept in statistics that underpins many statistical techniques and enables valid inference and analysis in a wide range of situations. It allows us to work with sample means and provides a bridge between the properties of sample statistics and population parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872427ae-3872-42f9-b25e-348bc50a8ffb",
   "metadata": {},
   "source": [
    "# Q10: State the assumptions of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef19122-eaef-47d3-a36a-c90385469b84",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a powerful statistical concept, but it relies on certain assumptions to hold. The assumptions of the Central Limit Theorem include:\n",
    "\n",
    "Independence: The random variables in the sample must be independent. This means that the value of one variable should not be influenced by or related to the value of another variable in the sample. Independence is a crucial assumption for the CLT to hold.\n",
    "\n",
    "Identically Distributed: The random variables in the sample must be drawn from the same population and have the same underlying distribution. They should follow the same probability distribution function with the same parameters. This assumption ensures that each variable contributes equally to the overall distribution of the sample mean.\n",
    "\n",
    "Finite Variance: The random variables in the population should have a finite variance. Variance measures the spread or dispersion of a random variable. For the CLT to hold, the individual random variables must have a finite variance. If the variance is infinite or undefined, the CLT may not apply.\n",
    "\n",
    "Sample Size: The CLT assumes that the sample size is sufficiently large. While there is no specific threshold for what constitutes a \"large\" sample size, as a general guideline, a sample size of at least 30 is often considered sufficient for the CLT to provide a reasonable approximation.\n",
    "\n",
    "It is important to note that violating these assumptions does not mean that the CLT cannot be applied at all. In some cases, relaxing or modifying the assumptions may still allow for the use of the CLT, but the resulting approximation may not be as accurate or reliable. Additionally, there are variations of the CLT, such as the Lindeberg-Levy CLT and Lyapunov CLT, that relax some of these assumptions under certain conditions.\n",
    "\n",
    "Overall, the assumptions of the Central Limit Theorem emphasize the importance of independence, identical distribution, and finite variance of the random variables being considered. These assumptions guide the applicability and reliability of the CLT in approximating the sampling distribution of the mean.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d5a9c-66ca-4b24-9206-a83a823a5c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
